---
title: "Influence du nombre d'abres"
output: html_document
---


```{r generate dist, echo=F}

library(randomForest)
library(ggplot2)
#la fonction generate forest génère une forêt de taille nTree à partir du jeu de données train
generate_forest=function(nTree, train){
  liste_arbres=list()
  mse=matrix(0, 1 , nTree)
  sum=matrix(0, 1,dim(test)[1])

  for (i in 1:nTree){
    fit <- randomForest(feature_to_predict ~ .,
                        data=train, importance=TRUE,  ntree=1, mtry=1)

    liste_arbres=c(liste_arbres, list(fit))
    sum=sum+predict(fit, test)
    pred=sum/i
    mse_arbre=mean((pred-test$feature_to_predict)^2)
    mse[1,i]=mse_arbre
  }
  #plot(1:nTree, mse)

  return (liste_arbres);
}


#à partir d'une liste d'arbres (ie une forêt), la fonction distance_matrix génère la matrice des distance entre les arbres
distance_matrix=function(liste_arbres){
  distance_matrix=matrix(0, nTree, nTree)
  l=1;
  m=1;
  for (i in liste_arbres){
    l=1
    for (j in liste_arbres){
      pred_1=predict(i, test)
      pred_2=predict(j, test)
      dist=mean((pred_2-pred_1)^2)
      distance_matrix[l,m]=dist
      l=l+1;
    }
    m=m+1
  }
  return (distance_matrix)
}


#plot_forest calcule à partir d'une forêt le risque quadratique sur successivement 1,2,3, ..., nTree arbres
#la méthode d'aggrégation dans la forêt est ici une moyenne
plot_forest=function(liste_arbres, test_set, nTree){
  test_set=test
  mse=matrix(0, 1 , nTree)
  sum=matrix(0, 1, dim(test)[1])

  for (i in 1:nTree){

    sum=sum+predict(liste_arbres[[i]], test_set)
    pred=sum/i
    mse_arbre=mean((pred-test_set$feature_to_predict)^2)
    mse[1,i]=mse_arbre
  }
  data=data.frame(cbind(t(t(1:nTree)), t(mse)))
  return(data)
  # return(ggplot(data, aes(x=X1, y=X2)))

}

```

```{r max_dist, echo=F}

max_dist=function(d, N,k){
  index <- -1 
  max_dis = -1
  res <- rep(0, k)
  for (p in 1:N){   
      num_chosen <- p
      
      chosen <- rep(0,N)
      chosen[num_chosen] <- 1
      
      a <- rep(0, k)
      a[1] <- num_chosen
      
      dis_to_set <-rep(0,N)
      
      for(i in 1:N){
          dis_to_set[i] <- d[i,num_chosen]
      }
      
      for( i in 2:k){
          far <- 1
          dist <- -1
          for( j in 1:N){
              if(chosen[j]==0 & dist<dis_to_set[j]){
                  dist <- dis_to_set[j]
                  far <- j
              }
          }
          a[i] <- far
          chosen[far]<-1
          for( j in 1:N){
            if(chosen[j]==0 ){
                dis_to_set[j] <- min(dis_to_set[j],d[j,far])
            }
          }
          
      }
      current_dist <- -1
      for ( i in 1:k){
          for(j in 1:k){
              if(d[a[i],a[j]]>current_dist){
                current_dist <- d[a[i],a[j]]
              }  
          }
      }
      if(current_dist>max_dis){
        max_dist <- current_dist
        index <- p
        for(m in 1:k){
          res[m] = a[m]
        }
      }
  }
  
  
  w =  rep(0, k)
  for (i in 1:N){
    k_min = 1
    dist_min = d[i,res[1]]
    for(j in 1:k){
      if(d[i,res[j]]<dist_min){
        k_min= j
        dist_min = d[i,res[j]]
      }
    }
    w[k_min] = w[k_min]+1;
  }
  for(i in 1:k){
    w[i] = w[i]/ N
  }
  return(list(res,w))
}


```


```{r chargement des données, echo=F}
ozone <- read.table("~/P3a/ozone.txt", quote="\"", comment.char="")
# source("/home/qbe/R/P3A/generate_dist.R")
# source("/home/qbe/R/P3A/max_dist.R")
library(gridExtra)

nTree=100
library(dummy)
ozone_dummy=dummy(ozone, int=TRUE)
ozone_dummy=cbind(ozone[,-c(12,13)], ozone_dummy)

ozone_dummy=data.frame(ozone_dummy)
n=dim(ozone_dummy)[1]
set.seed(5)
bool=runif(n)<0.30

n=which(names(ozone_dummy)=="maxO3")
names(ozone_dummy)[n]="feature_to_predict"
test=ozone_dummy[bool,]
train=ozone_dummy[!bool,]
liste_arbres=generate_forest(nTree, train)
```

Nous avons tout d'abord tracé la courbe erreur quadratique en fonction du nombre d'arbres dans la forêt d'un random forest pour quelques jeux de données, des jeux de données simulés et le jeu de données ozone.

On a pu observer que le nombre d'arbres n'améliorait pas la prédiction au delà d'un certain seuil.
On a donc, comme évoqué dans la réunion, tenté de choisir des arbres assez différents puis tenté d'effectuer une pondération en fonction de la représentativité des arbres sélectionnés.
Le résultat de la pondération est satisfaisant sans être exceptionnel, c'est-à-dire qu'il n'améliore pas toujours la prédiction de manière transcendante (c'est-à-dire que la différence entre prédire avec un certain nombre d'arbres bien choisis et prédire sur un même nombre d'arbres choisis aléatoirement n'est pas toujours importante.

```{r foret initiale, fig.width=12}

g1=ggplot(plot_forest(liste_arbres, test, nTree), aes(X1, X2))+geom_point()+xlab(label="nombre d'arbres")+ylab(label="mse")+
  labs(title="diminution de l'erreur quadratique en fonction du nombre d'arbres")#on stocke le graphe

g1
```

On voit ici que la forêt d'arbre n'est trop grande, 20 arbres suffiraient à prédire correctement.


## Détails techniques du choix des arbres : 
Nous appliquons l'algorithme random forest, nous nous retrouvons donc avec une forêt d'arbres, dont nous savons que beaucoup d'arbres sont redondants. L'idée était de choisir seulement certains arbres (par exemple 15 ou 20) pour retrouver la propriété d'interprétabilité facile des arbres, si possible les arbres les plus différents possibles.


## La question qui vient alors est choisir des arbres "différents" en quel sens ?

Nous avons pris, dans un premier temps, la distance :

### Distance

moyenne_quadratique[(preiction - valeur_réelle)(sur un échantillon test indépendant du jeu d'entrainement)]

Une fois cette distance définie nous pouvons calculer la matrice des distances entre chaque arbres.


### Alglorithme de sélection

Pour séléctionner les arbres à garder, nous avons appliquer un algorithme qui permet de sélectionner les arbres les plus éloignés au sens de la ditance définie plus haut (une heuristique du problème original NP-complet).
Puis nous regardons, pour tous les autres arbres de la forêt, de quel arbre ils sont le plus proches, et pondérons en conséquence chacun des arbres sélectionnés.

Nous avons tracé le résultat pour un jeu de données simulées, puis pour le jeu de données ozone (on prédit maxO3).


```{r foret reduite, fig.width=12}
d=distance_matrix(liste_arbres)


k = 15 #nmobre d'arbres que l'on souhaite garder dans la foret
res=max_dist(d, nTree, k) #indice des arbres à garder dans la foret initiale
tab_indices=res[[1]]
poids=res[[2]]

liste_arbres_distincts=NULL #on récupère les arbres à garder
for (i in tab_indices){
  print(i)
  liste_arbres_distincts=c(liste_arbres_distincts, list(liste_arbres[[i]]))
}

g2=ggplot(plot_forest(liste_arbres_distincts, test, k), aes(x = X1, y=X2))+geom_point()+xlab(label="nombre d'arbres")+ylab(label="mse")+
  labs(title="diminution de l'erreur quadratique en fonction du nombre d'arbres, arbres sélectionnés, non pondérés")
#g2
#grid.arrange(g1,g2)


#on veut voir le résultat avec pondération
pred=0;
for (i in 1:k){
  pred=pred+predict(liste_arbres_distincts[[i]], test)*poids[i]
}

#on voit que l'erreur quadratique est plus petite que dans la forêt classique
mean((pred-test$feature_to_predict)^2)


#ploter le tout sur un même graphe
res_foret_initiale=plot_forest(liste_arbres, test, nTree)
res_foret_initiale$fill="forêt initiale"

res_foret_selectionnee=plot_forest(liste_arbres_distincts, test, k)
res_foret_selectionnee$fill="forêt arbres différents, non pondérés"
data_to_plot=rbind(res_foret_initiale, res_foret_selectionnee)
data_to_plot=rbind(data_to_plot, c(k, mean((pred-test$feature_to_predict)^2), "forêt arbre différents pondérés"))
data_to_plot$X1=as.numeric(data_to_plot$X1)
data_to_plot$X2=as.numeric(data_to_plot$X2)

g3=ggplot(data_to_plot, aes(x=X1, y=X2, color=fill))+geom_point()+xlab(label="nombre d'arbres")+ylab(label="mse")+
  labs(title="diminution de l'erreur quadratique en fonction du nombre d'arbres")

g3

print(poids)
```

## Axe de travail

Nous pensons que les principaux prochains axes de travail vont être de tester différentes distances et différents algorithmes pour la sélection des arbres dans la forêt. En particulier nous avons deux choses en tête :


* quand on predit à l'aide d'un arbre sur un jeu de données test, on peut voir l'arbre comme un vecteur de dimension la taille du jeu de donée (chaque coordonnées étant la prédiction d'un échantillon), à partir de là on peut appliquer l'algorithme des k-moyennes ou d'autres algorithmes de clusterisation pour regrouper les arbres les plus proches (ici nous choisirions la distance 

* un des aspects étrange de la distance choisie jusqu'ici est qu'elle se base sur un jeu de données, on évalue un jeu de données sur chacun des arbres, puis on compare les résultats. Nous pensions qu'un axe intéressant de reflexion pourrait être d'évaluer la distance entre les arbres de manière intrinsèques, sans jeux de données extérieurs. En effet chaque arbre peut être considéré comme une fonction de R^d (d=nombre de paramètre par échantillon), à valeur réelle, qui, de plus, est constante par morceaux. À partir de ça il doit être possible de trouver une distance maligne, intrinsèque, (sans jeux de données), comme par exemple la norme L2 sur un certain compact bien choisi.


* étendre le travail à d'autres jeux de données, notamment de classification



## Questions

nous sommes preneurs de retours, notamment de votre avis sur les prochaines axes de travail, et de conseils sur des jeux de données intéressants en regression, ou toutes autres remarques telles qu'elles soient.